{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coronagraph inpainting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data downloading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/paramiko/transport.py:237: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "Importing data\n",
      "LASCO-C2:22157691.fts:2003/10/28T20:10:35.655...\n",
      "Importing data\n",
      "LASCO-C2:22157091.fts:2003/10/20T01:55:05.979...\n",
      "Importing data\n",
      "LASCO-C2:22157383.fts:2003/10/24T06:54:30.881...\n",
      "Importing data\n",
      "LASCO-C2:22157143.fts:2003/10/20T20:54:05.924...\n",
      "Importing data\n",
      "LASCO-C2:22157509.fts:2003/10/26T02:30:30.992...\n",
      "Importing data\n",
      "LASCO-C2:22157573.fts:2003/10/27T00:06:05.441...\n",
      "Importing data\n",
      "LASCO-C2:22157708.fts:2003/10/29T05:24:10.719...\n",
      "Importing data\n",
      "LASCO-C2:22157415.fts:2003/10/24T18:06:05.473...\n",
      "Importing data\n",
      "LASCO-C2:22157428.fts:2003/10/24T21:54:05.535...\n",
      "Importing data\n",
      "LASCO-C2:22157332.fts:2003/10/23T13:33:53.413...\n",
      "Importing data\n",
      "LASCO-C2:22157423.fts:2003/10/24T20:56:39.970...\n",
      "Importing data\n",
      "LASCO-C2:22157809.fts:2003/10/30T22:09:44.810...\n",
      "Importing data\n",
      "LASCO-C2:22157707.fts:2003/10/29T04:15:34.061...\n",
      "Importing data\n",
      "LASCO-C2:22157777.fts:2003/10/30T10:23:04.539...\n",
      "Importing data\n",
      "LASCO-C2:22157742.fts:2003/10/29T19:45:07.897...\n",
      "Importing data\n",
      "LASCO-C2:22157248.fts:2003/10/22T08:54:05.709...\n",
      "Importing data\n",
      "LASCO-C2:22157299.fts:2003/10/23T02:06:05.781...\n",
      "Importing data\n",
      "LASCO-C2:22157221.fts:2003/10/21T22:30:05.689...\n",
      "Importing data\n",
      "LASCO-C2:22157442.fts:2003/10/25T02:58:47.750...\n",
      "Importing data\n",
      "LASCO-C2:22157148.fts:2003/10/20T21:30:05.603...\n",
      "Importing data\n",
      "LASCO-C2:22157476.fts:2003/10/25T15:30:05.493...\n",
      "Importing data\n",
      "LASCO-C2:22157224.fts:2003/10/22T00:06:05.431...\n",
      "Importing data\n",
      "LASCO-C2:22157407.fts:2003/10/24T15:30:05.468...\n",
      "Importing data\n",
      "LASCO-C2:22157334.fts:2003/10/23T14:31:44.478...\n",
      "Importing data\n",
      "LASCO-C2:22157730.fts:2003/10/29T15:54:50.437...\n",
      "Importing data\n",
      "LASCO-C2:22157365.fts:2003/10/24T00:30:05.414...\n",
      "Importing data\n",
      "LASCO-C2:22157336.fts:2003/10/23T15:06:05.457...\n",
      "Importing data\n",
      "LASCO-C2:22157278.fts:2003/10/22T19:31:38.621...\n",
      "Importing data\n",
      "LASCO-C2:22157606.fts:2003/10/27T11:54:05.973...\n",
      "Importing data\n",
      "LASCO-C2:22157518.fts:2003/10/26T05:30:05.583...\n",
      "Importing data\n",
      "LASCO-C2:22157109.fts:2003/10/20T08:06:18.354...\n",
      "Importing data\n",
      "LASCO-C2:22157433.fts:2003/10/24T23:54:05.461...\n",
      "Importing data\n",
      "LASCO-C2:22157501.fts:2003/10/25T23:30:06.901...\n",
      "Importing data\n",
      "LASCO-C2:22157393.fts:2003/10/24T10:30:21.250...\n",
      "Importing data\n",
      "LASCO-C2:22157250.fts:2003/10/22T09:30:05.387...\n",
      "Importing data\n",
      "LASCO-C2:22157150.fts:2003/10/20T22:06:05.581...\n",
      "Importing data\n",
      "LASCO-C2:22157395.fts:2003/10/24T11:06:05.528...\n",
      "Importing data\n",
      "LASCO-C2:22157432.fts:2003/10/24T23:30:05.477...\n",
      "Importing data\n",
      "LASCO-C2:22157512.fts:2003/10/26T03:00:33.874...\n",
      "Importing data\n",
      "LASCO-C2:22157738.fts:2003/10/29T18:22:03.047...\n",
      "Importing data\n",
      "LASCO-C2:22157259.fts:2003/10/22T12:30:05.678...\n",
      "Importing data\n",
      "LASCO-C2:22157626.fts:2003/10/27T19:54:05.440...\n",
      "Importing data\n",
      "LASCO-C2:22157188.fts:2003/10/21T11:54:05.677...\n",
      "Importing data\n",
      "LASCO-C2:22157745.fts:2003/10/29T20:56:38.029...\n",
      "Importing data\n",
      "LASCO-C2:22157329.fts:2003/10/23T12:30:05.702...\n",
      "Importing data\n",
      "LASCO-C2:22157276.fts:2003/10/22T18:30:05.759...\n",
      "Importing data\n",
      "LASCO-C2:22157733.fts:2003/10/29T16:51:04.403...\n",
      "Importing data\n",
      "LASCO-C2:22157542.fts:2003/10/26T14:06:05.669...\n",
      "Importing data\n",
      "LASCO-C2:22157685.fts:2003/10/28T16:51:02.377...\n",
      "Importing data\n",
      "LASCO-C2:22157451.fts:2003/10/25T06:30:06.521...\n",
      "Importing data\n",
      "LASCO-C2:22157114.fts:2003/10/20T09:54:32.388...\n",
      "Importing data\n",
      "LASCO-C2:22157292.fts:2003/10/22T23:30:05.476...\n",
      "Importing data\n",
      "LASCO-C2:22157715.fts:2003/10/29T09:58:37.653...\n",
      "Importing data\n",
      "LASCO-C2:22157095.fts:2003/10/20T03:06:18.137...\n",
      "Importing data\n",
      "LASCO-C2:22157168.fts:2003/10/21T04:54:05.532...\n",
      "Importing data\n",
      "LASCO-C2:22157734.fts:2003/10/29T17:14:04.189...\n",
      "Importing data\n",
      "LASCO-C2:22157481.fts:2003/10/25T17:06:06.435...\n",
      "Importing data\n",
      "LASCO-C2:22157629.fts:2003/10/27T20:54:07.305...\n",
      "Importing data\n",
      "LASCO-C2:22157460.fts:2003/10/25T09:54:05.498...\n",
      "Importing data\n",
      "LASCO-C2:22157765.fts:2003/10/30T04:37:27.349...\n",
      "Importing data\n",
      "LASCO-C2:22157690.fts:2003/10/28T19:27:44.881...\n",
      "Importing data\n",
      "LASCO-C2:22157677.fts:2003/10/28T12:54:05.521...\n",
      "Importing data\n",
      "LASCO-C2:22157104.fts:2003/10/20T06:06:21.527...\n",
      "Importing data\n",
      "LASCO-C2:22157286.fts:2003/10/22T21:08:04.963...\n",
      "Importing data\n",
      "LASCO-C2:22157330.fts:2003/10/23T12:54:05.487...\n",
      "Importing data\n",
      "LASCO-C2:22157634.fts:2003/10/27T21:30:05.582...\n",
      "Importing data\n",
      "LASCO-C2:22157211.fts:2003/10/21T20:06:05.577...\n",
      "Importing data\n",
      "LASCO-C2:22157514.fts:2003/10/26T03:30:12.855...\n",
      "Importing data\n",
      "LASCO-C2:22157394.fts:2003/10/24T10:54:05.436...\n",
      "Importing data\n",
      "LASCO-C2:22157565.fts:2003/10/26T21:08:49.712...\n",
      "Importing data\n",
      "LASCO-C2:22157632.fts:2003/10/27T21:04:17.398...\n",
      "Importing data\n",
      "LASCO-C2:22157335.fts:2003/10/23T14:54:05.464...\n",
      "Importing data\n",
      "LASCO-C2:22157795.fts:2003/10/30T17:34:08.877...\n",
      "Importing data\n",
      "LASCO-C2:22157350.fts:2003/10/23T20:06:05.475...\n",
      "Importing data\n",
      "LASCO-C2:22157180.fts:2003/10/21T09:06:07.080...\n",
      "Importing data\n",
      "LASCO-C2:22157533.fts:2003/10/26T10:56:19.185...\n",
      "Importing data\n",
      "FITS file doesn't contain image data, avorting and deleting...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jenci/Desktop/DL-based-Coronagraph-Inpainting/data.py\", line 70, in <module>\n",
      "    downloader.level_1()\n",
      "  File \"/home/jenci/Desktop/DL-based-Coronagraph-Inpainting/data.py\", line 66, in level_1\n",
      "    plt.imshow(fits.getdata(path))\n",
      "  File \"/home/jenci/.local/lib/python3.10/site-packages/astropy/io/fits/convenience.py\", line 220, in getdata\n",
      "    hdulist, extidx = _getext(filename, mode, *args, **kwargs)\n",
      "  File \"/home/jenci/.local/lib/python3.10/site-packages/astropy/io/fits/convenience.py\", line 1114, in _getext\n",
      "    hdulist = fitsopen(filename, mode=mode, **kwargs)\n",
      "  File \"/home/jenci/.local/lib/python3.10/site-packages/astropy/io/fits/hdu/hdulist.py\", line 222, in fitsopen\n",
      "    return HDUList.fromfile(\n",
      "  File \"/home/jenci/.local/lib/python3.10/site-packages/astropy/io/fits/hdu/hdulist.py\", line 486, in fromfile\n",
      "    return cls._readfrom(\n",
      "  File \"/home/jenci/.local/lib/python3.10/site-packages/astropy/io/fits/hdu/hdulist.py\", line 1157, in _readfrom\n",
      "    fileobj = _File(\n",
      "  File \"/home/jenci/.local/lib/python3.10/site-packages/astropy/io/fits/file.py\", line 218, in __init__\n",
      "    self._open_filename(fileobj, mode, overwrite)\n",
      "  File \"/home/jenci/.local/lib/python3.10/site-packages/astropy/io/fits/file.py\", line 641, in _open_filename\n",
      "    self._file = open(self.name, IO_FITS_MODES[mode])\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/jenci/Desktop/DL-based-Coronagraph-Inpainting/data/c2/20031022_113005.fits'\n"
     ]
    }
   ],
   "source": [
    "!python3 data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from utils import get_default_device\n",
    "device = get_default_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/paramiko/transport.py:237: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "from utils import DeviceDataLoader\n",
    "from data import CoronagraphDataset\n",
    "\n",
    "dataset = CoronagraphDataset('c2')\n",
    "# 0.8 - 0.2\n",
    "train_len = round(0.8*len(dataset))\n",
    "val_len = len(dataset) - train_len\n",
    "\n",
    "#random split\n",
    "train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
    "\n",
    "train_dl = DeviceDataLoader(DataLoader(train_ds, batch_size, True, num_workers = 4, pin_memory=True), device)\n",
    "val_dl = DeviceDataLoader(DataLoader(val_ds, batch_size*2, True, num_workers = 4, pin_memory=True), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CoronagraphReconstructor\n",
    "from utils import to_device\n",
    "\n",
    "model = to_device(CoronagraphReconstructor('attempt_1', in_channels = 1, deep = 2, layers = 2), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 0:   0%|          | 0/194 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 11.72 GiB of which 319.69 MiB is free. Including non-PyTorch memory, this process has 10.92 GiB memory in use. Of the allocated memory 10.71 GiB is allocated by PyTorch, and 26.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneCycleLR\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_sched\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mOneCycleLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msaving_div\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DL-based-Coronagraph-Inpainting/model.py:234\u001b[0m, in \u001b[0;36mTrainingPhase.fit\u001b[0;34m(self, train_loader, val_loader, epochs, batch_size, lr, weight_decay, grad_clip, opt_func, lr_sched, saving_div, graph, sample_input)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m#training step\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_sched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n",
      "File \u001b[0;32m~/Desktop/DL-based-Coronagraph-Inpainting/model.py:89\u001b[0m, in \u001b[0;36mTrainingPhase.training_step\u001b[0;34m(self, batch, lr_sched, grad_clip)\u001b[0m\n\u001b[1;32m     87\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclone(ground_truth)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork):\n\u001b[0;32m---> 89\u001b[0m     x, updated_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     L_hole, L_valid, L_perceptual, \\\n\u001b[1;32m     91\u001b[0m         L_style_out, L_style_comp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(x, ground_truth, prior_mask, updated_mask)\n\u001b[1;32m     92\u001b[0m     loss \u001b[38;5;241m=\u001b[39m L_hole \u001b[38;5;241m+\u001b[39m L_valid \u001b[38;5;241m+\u001b[39m L_perceptual \u001b[38;5;241m+\u001b[39m L_style_comp \u001b[38;5;241m+\u001b[39m L_style_out\n",
      "File \u001b[0;32m~/Desktop/DL-based-Coronagraph-Inpainting/model.py:308\u001b[0m, in \u001b[0;36mCoronagraphReconstructor.single_forward\u001b[0;34m(self, input, mask_im, *args)\u001b[0m\n\u001b[1;32m    306\u001b[0m x, mask \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m](\u001b[38;5;28minput\u001b[39m, mask_im)\n\u001b[1;32m    307\u001b[0m x, mask \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m](x, mask)\n\u001b[0;32m--> 308\u001b[0m x, mask \u001b[38;5;241m=\u001b[39m \u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m x \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m3\u001b[39m](x)\n\u001b[1;32m    310\u001b[0m x, mask\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m4\u001b[39m](x, mask)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DL-based-Coronagraph-Inpainting/model.py:279\u001b[0m, in \u001b[0;36mSingleResidualBlock.forward\u001b[0;34m(self, input, mask_in)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, mask_in: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[0;32m--> 279\u001b[0m     x, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_res_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m, mask\n",
      "File \u001b[0;32m~/Desktop/DL-based-Coronagraph-Inpainting/model.py:271\u001b[0m, in \u001b[0;36mSingleResidualBlock._res_conv_forward\u001b[0;34m(self, input, mask_in)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_res_conv_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, mask_in: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m partial_conv, activation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv:\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28minput\u001b[39m, mask_in \u001b[38;5;241m=\u001b[39m \u001b[43mpartial_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m activation(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m, mask_in\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DL-based-Coronagraph-Inpainting/partial_conv.py:42\u001b[0m, in \u001b[0;36mPartialConv2d.forward\u001b[0;34m(self, input, mask_in)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, mask_in: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[Tensor, Tensor], Tensor]:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_part_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_in\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DL-based-Coronagraph-Inpainting/partial_conv.py:37\u001b[0m, in \u001b[0;36mPartialConv2d._part_conv_forward\u001b[0;34m(self, input, mask_in)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_1 \u001b[38;5;241m/\u001b[39m (sum_m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)) \u001b[38;5;241m+\u001b[39m bias\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, updated_mask \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_mask \u001b[38;5;28;01melse\u001b[39;00m out\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 11.72 GiB of which 319.69 MiB is free. Including non-PyTorch memory, this process has 10.92 GiB memory in use. Of the allocated memory 10.71 GiB is allocated by PyTorch, and 26.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim import Adam\n",
    "\n",
    "model.fit(\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    epochs = 100,\n",
    "    lr = 1e-2,\n",
    "    batch_size = batch_size,\n",
    "    weight_decay = 1e-6,\n",
    "    grad_clip = 1e-1,\n",
    "    opt_func = Adam,\n",
    "    lr_sched = OneCycleLR,\n",
    "    saving_div = 15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
